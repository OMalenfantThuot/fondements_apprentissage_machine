{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devoir 3 pour IFT6390 - Fondements de l'apprentissage machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Par Olivier Malenfant-Thuot\n",
    "Matricule: 1012818"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Theoretical Part: Derivatives and relationships between basic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{2}(\\tanh(x/2)+1) = \\frac{1}{2}(\\frac{e^{x/2}-e^{-x/2}}{e^{x/2}+e^{-x/2}}+1) = \n",
    "\\frac{1}{2}(\\frac{e^{x/2}-e^{-x/2}}{e^{x/2}+e^{-x/2}}+\\frac{e^{x/2}+e^{-x/2}}{e^{x/2}+e^{-x/2}}) = \\frac{1}{2}(\\frac{2e^{x/2}}{e^{x/2}+e^{-x/2}}) = \\frac{2e^{x/2}(e^{x/2}+e^{-x/2})}{(e^{x/2}+e^{-x/2})^2} = \\frac{e^x+1}{e^x+2+e^{-x}} \\times \\frac{(e^x+1)^{-1}}{(e^x+1)^{-1}} = \\frac{1}{1+e^{-x}} = \\text{sigmoid}(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "\\begin{equation}\n",
    "\\ln(\\text{sigmoid}(x)) = \\ln(\\frac{1}{1+e^{-x}}) = \\ln(1) - \\ln(1+e^{-x}) = 0-\\ln(1+e^{-x})=-\\ln(1+e^{-x}) = -\\text{softplus}(-x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial x}\\text{sigmoid}(x) = \\frac{\\partial}{\\partial x}\\frac{1}{1+e^{-x}} = -(1+e^{-x})^{-2}(-e^{-x}) = \\frac{e^{-x}}{(1+e^{-x})^2} = \\frac{1}{1+e^{-x}}\\frac{e^{-x}}{1+e^{-x}} = \\frac{1}{1+e^{-x}}\\frac{1-1+e^{-x}}{1+e^{-x}} = \\frac{1}{1+e^{-x}}(\\frac{1+e^{-x}}{1+e^{-x}} - \\frac{1}{1+e^{-x}}) = \\frac{1}{1+e^{-x}}(1-\\frac{1}{1+e^{-x}}) =\\text{sigmoid}(x) (1 - \\text{sigmoid}(x))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial x}(\\tanh(x)) = \\frac{\\partial}{\\partial x} (\\frac{e^x - e^{-x}}{e^x + e^{-x}}) = \\frac{e^x + e^{-x}}{e^x + e^{-x}} - (e^x - e^{-x})(e^x + e^{-x})^{-2}(e^x - e^{-x}) = 1 - (\\frac{e^x - e^{-x}}{e^x + e^{-x}})^2 = 1 - \\tanh(x)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{sign}(x) = 1_{x>0}(x) - 1_{x<0}(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{abs'}(x) = \\text{sign(x)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{rect'}(x) = 1_{x>0}(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L_2}{\\partial x} = \\begin{bmatrix}\\frac{\\partial}{\\partial x_1}\\\\\n",
    "\\frac{\\partial}{\\partial x_2}\\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial x_d}\\end{bmatrix} L_2 = \\begin{bmatrix}2x_1\\\\\n",
    "2x_2\\\\\n",
    "\\vdots \\\\\n",
    "2x_d\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L_1}{\\partial x} = \\begin{bmatrix}\\frac{\\partial}{\\partial x_1}\\\\\n",
    "\\frac{\\partial}{\\partial x_2}\\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial x_d}\\end{bmatrix} L_1 = \\begin{bmatrix}1\\\\\n",
    "1\\\\\n",
    "\\vdots \\\\\n",
    "1\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Gradient computation for parameters optimization in a neural net for multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "Le vecteur $b^{(1)}$ est de dimensions $d_h$.\n",
    "\n",
    "Le vecteur de préactivation de la couche cachée est donné par\n",
    "\n",
    "\\begin{equation}\n",
    "h^{(a)}(x) = b^{(1)} + W^{(1)}h^{(0)}(x)\n",
    "\\end{equation}\n",
    "\n",
    "avec $h^{(0)}(x) = x$.\n",
    "\n",
    "Pour calculer un élément particulier de rang $j$, on utilise\n",
    "\n",
    "\\begin{equation}\n",
    "h^{(a)}_j(x) =  b^{(1)}_j + \\sum\\limits_{i = 1}^d W^{(1)}_{ji}x_i.\n",
    "\\end{equation}\n",
    "\n",
    "Les éléments du  vecteur de sortie pour la couche cachée, $h^{(s)}(x)$ sont donnés par\n",
    "\n",
    "\\begin{equation}\n",
    "h^{(s)}_j(x) = \\max(0,h^{(a)}_j(x)).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "Les dimensions de la matrice $W^{(2)}$ est $(m,d_h)$ et celles du vecteur $b^{(2)}$ sont $(m,1)$. La formule d'activation de la couche de sortie est donnée par\n",
    "\n",
    "\\begin{equation}\n",
    "o^{(a)}(x) = b^{(2)} + W^{(2)}h^{(s)}(x).\n",
    "\\end{equation}\n",
    "\n",
    "Chaque élément $o^{(a)}_k(x)$ de ce vecteur est donné par\n",
    "\n",
    "\\begin{equation}\n",
    "o^{(a)}_k(x) = b^{(2)}_k + \\sum\\limits_{j = 1}^{d_h} W^{(2)}_{kj}h^{(s)}_j(x).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "Les éléments du vecteur de sortie $o^{(s)}_k(x)$ sont donnés par\n",
    "\n",
    "\\begin{equation}\n",
    "o^{(s)}_k(x) = \\text{softmax}(o^{(a)}_k(x)) = \\ln(1 + \\exp(o^{(a)}_k(x))) = \\ln(1 + \\exp(b^{(2)}_k + \\sum\\limits_{j = 1}^{d_h} W^{(2)}_{kj}h^{(s)}_j(x))).\n",
    "\\end{equation}\n",
    "\n",
    "Les $o^{(s)}_k(x)$ sont tous positifs, car $(\\exp(x) > 0 \\text{ pour } x \\in \\rm I\\!R)$, donc $1 + \\exp(x) > 1$, donc $\\ln(1+\\exp(x)) > 0$, car $\\ln(x) > 0\\text{ pour } x > 1$.\n",
    "\n",
    "???Normalisation???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "La fonction de perte est donnée par\n",
    "\n",
    "\\begin{equation}\n",
    "L(x) = \\sum\\limits_{y=1}^m L(x,y) = -\\sum\\limits_{y=1}^m \\ln(\\ln(1+\\exp(o^{(a)}_y(x))))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "\n",
    "Le risque empirique $\\hat{R}(x)$ du dataset $D_n$ est donné par\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{R}(x) = \\sum\\limits_{x \\in D_n} L(x) + \\lambda\\Omega(\\theta).\n",
    "\\end{equation}\n",
    "\n",
    "L'ensemble $\\theta$ des paramètres du système est \n",
    "\n",
    "\\begin{equation}\n",
    "\\theta = \\{W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)}\\}.\n",
    "\\end{equation}\n",
    "\n",
    "???Paramètres scalaires???\n",
    "\n",
    "Les valeurs de paramètres optimisées seront obtenues par\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^* = \\text{argmin}_{\\theta}(-\\sum\\limits_{x \\in D_n} \\sum\\limits_{y=1}^m \\ln(\\ln(1+\\exp(o^{(a)}_y(x)))) + \\lambda\\Omega(\\theta))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
