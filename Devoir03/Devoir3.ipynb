{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devoir 3 pour IFT6390 - Fondements de l'apprentissage machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Par Olivier Malenfant-Thuot\n",
    "Matricule: 1012818"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Theoretical Part: Derivatives and relationships between basic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{2}(\\tanh(x/2)+1) = \\frac{1}{2}(\\frac{e^{x/2}-e^{-x/2}}{e^{x/2}+e^{-x/2}}+1) = \n",
    "\\frac{1}{2}(\\frac{e^{x/2}-e^{-x/2}}{e^{x/2}+e^{-x/2}}+\\frac{e^{x/2}+e^{-x/2}}{e^{x/2}+e^{-x/2}}) = \\frac{1}{2}(\\frac{2e^{x/2}}{e^{x/2}+e^{-x/2}}) = \\frac{2e^{x/2}(e^{x/2}+e^{-x/2})}{(e^{x/2}+e^{-x/2})^2} = \\frac{e^x+1}{e^x+2+e^{-x}} \\times \\frac{(e^x+1)^{-1}}{(e^x+1)^{-1}} = \\frac{1}{1+e^{-x}} = \\text{sigmoid}(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "\\begin{equation}\n",
    "\\ln(\\text{sigmoid}(x)) = \\ln(\\frac{1}{1+e^{-x}}) = \\ln(1) - \\ln(1+e^{-x}) = 0-\\ln(1+e^{-x})=-\\ln(1+e^{-x}) = -\\text{softplus}(-x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial x}\\text{sigmoid}(x) = \\frac{\\partial}{\\partial x}\\frac{1}{1+e^{-x}} = -(1+e^{-x})^{-2}(-e^{-x}) = \\frac{e^{-x}}{(1+e^{-x})^2} = \\frac{1}{1+e^{-x}}\\frac{e^{-x}}{1+e^{-x}} = \\frac{1}{1+e^{-x}}\\frac{1-1+e^{-x}}{1+e^{-x}} = \\frac{1}{1+e^{-x}}(\\frac{1+e^{-x}}{1+e^{-x}} - \\frac{1}{1+e^{-x}}) = \\frac{1}{1+e^{-x}}(1-\\frac{1}{1+e^{-x}}) =\\text{sigmoid}(x) (1 - \\text{sigmoid}(x))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial x}(\\tanh(x)) = \\frac{\\partial}{\\partial x} (\\frac{e^x - e^{-x}}{e^x + e^{-x}}) = \\frac{e^x + e^{-x}}{e^x + e^{-x}} - (e^x - e^{-x})(e^x + e^{-x})^{-2}(e^x - e^{-x}) = 1 - (\\frac{e^x - e^{-x}}{e^x + e^{-x}})^2 = 1 - \\tanh(x)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{sign}(x) = 1_{x>0}(x) - 1_{x<0}(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{abs'}(x) = \\text{sign(x)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{rect'}(x) = 1_{x>0}(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L_2}{\\partial x} = \\begin{bmatrix}\\frac{\\partial}{\\partial x_1}\\\\\n",
    "\\frac{\\partial}{\\partial x_2}\\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial x_d}\\end{bmatrix} L_2 = \\begin{bmatrix}2x_1\\\\\n",
    "2x_2\\\\\n",
    "\\vdots \\\\\n",
    "2x_d\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L_1}{\\partial x} = \\begin{bmatrix}\\frac{\\partial}{\\partial x_1}\\\\\n",
    "\\frac{\\partial}{\\partial x_2}\\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial x_d}\\end{bmatrix} L_1 = \\begin{bmatrix}-1_{x<0}(x) + 1_{x>0}(x)\\\\\n",
    "-1_{x<0}(x) + 1_{x>0}(x)\\\\\n",
    "\\vdots \\\\\n",
    "-1_{x<0}(x) + 1_{x>0}(x)\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Gradient computation for parameters optimization in a neural net for multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "Le vecteur $b^{(1)}$ est de dimensions $d_h$.\n",
    "\n",
    "Le vecteur de préactivation de la couche cachée est donné par\n",
    "\n",
    "\\begin{equation}\n",
    "h^{(a)}(x) = b^{(1)} + W^{(1)}h^{(0)}(x)\n",
    "\\end{equation}\n",
    "\n",
    "avec $h^{(0)}(x) = x$.\n",
    "\n",
    "Pour calculer un élément particulier de rang $j$, on utilise\n",
    "\n",
    "\\begin{equation}\n",
    "h^{(a)}_j(x) =  b^{(1)}_j + \\sum\\limits_{i = 1}^d W^{(1)}_{ji}x_i.\n",
    "\\end{equation}\n",
    "\n",
    "Les éléments du  vecteur de sortie pour la couche cachée, $h^{(s)}(x)$ sont donnés par\n",
    "\n",
    "\\begin{equation}\n",
    "h^{(s)}_j(x) = \\max(0,h^{(a)}_j(x)).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "Les dimensions de la matrice $W^{(2)}$ est $(m,d_h)$ et celles du vecteur $b^{(2)}$ sont $(m,1)$. La formule d'activation de la couche de sortie est donnée par\n",
    "\n",
    "\\begin{equation}\n",
    "o^{(a)}(x) = b^{(2)} + W^{(2)}h^{(s)}(x).\n",
    "\\end{equation}\n",
    "\n",
    "Chaque élément $o^{(a)}_k(x)$ de ce vecteur est donné par\n",
    "\n",
    "\\begin{equation}\n",
    "o^{(a)}_k(x) = b^{(2)}_k + \\sum\\limits_{j = 1}^{d_h} W^{(2)}_{kj}h^{(s)}_j(x).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "Les éléments du vecteur de sortie $o^{(s)}_k(x)$ sont donnés par\n",
    "\n",
    "\\begin{equation}\n",
    "o^{(s)}_k(x) = \\text{softmax}(o^{(a)}_k(x)) = \\ln(1 + \\exp(o^{(a)}_k(x))) = \\ln(1 + \\exp(b^{(2)}_k + \\sum\\limits_{j = 1}^{d_h} W^{(2)}_{kj}h^{(s)}_j(x))).\n",
    "\\end{equation}\n",
    "\n",
    "Les $o^{(s)}_k(x)$ sont tous positifs, car $(\\exp(x) > 0 \\text{ pour } x \\in \\rm I\\!R)$, donc $1 + \\exp(x) > 1$, donc $\\ln(1+\\exp(x)) > 0$, car $\\ln(x) > 0\\text{ pour } x > 1$.\n",
    "\n",
    "???Normalisation???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "La fonction de perte est donnée par\n",
    "\n",
    "\\begin{equation}\n",
    "L(x,y) = -\\ln(\\ln(1+\\exp(o^{(a)}(x)\\times \\text{onehot}_m(y))))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "\n",
    "Le risque empirique $\\hat{R}(x)$ du dataset $D_n$ est donné par\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{R}(x) = \\sum\\limits_{x \\in D_n} \\sum\\limits_{y =1}^m L(x,y).\n",
    "\\end{equation}\n",
    "\n",
    "L'ensemble $\\theta$ des paramètres du système est \n",
    "\n",
    "\\begin{equation}\n",
    "\\theta = \\{W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)}\\}.\n",
    "\\end{equation}\n",
    "\n",
    "$W^{(1)}$ est de dimension $(d_h,d)$, $b^{(1)}$ est de dimension $d_h$, $W^{(2)}$ est de dimension $(m,d_h)$ et $b^{(2)}$ est de dimension $m$. Le nombre de paramètres scalaires total est donc $n_\\theta = d_h(d+1) + m(d_h+1)$\n",
    "\n",
    "Les valeurs de paramètres optimisées seront obtenues par\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^* = \\text{argmin}_{\\theta}(-\\sum\\limits_{x \\in D_n} \\sum\\limits_{y=1}^m \\ln(\\ln(1+\\exp(o^{(a)}_y(x)\\times \\text{onehot}_m(y)))))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\n",
    "\n",
    "L'équation pour le batch gradient descent est\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla(\\theta) = \\frac{1}{n} \\sum\\limits_{x\\in D_n}[\\frac{\\partial}{\\partial \\theta}\\sum\\limits_{y=1}^m -\\ln(\\ln(1+\\exp(o^{(a)}_y(x)\\times \\text{onehot}_m(y)))) ],\\\\\n",
    "\\theta \\leftarrow \\theta - \\eta\\nabla(\\theta).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\n",
    "\n",
    "À voir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\n",
    "\n",
    "```python\n",
    "grad_oa = np.copy(os)\n",
    "grad_oa[target == 1] -= 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{(2)}_{kj}} = \\frac{\\partial L}{\\partial o^{(a)}_{k}} \\frac{\\partial o^{(a)}_{k}}{\\partial W^{(2)}_{kj}} = \\frac{\\partial L}{\\partial o^{(a)}_{k}} h^{(s)}_j\\\\\n",
    "\\frac{\\partial L}{\\partial b^{(2)}_{k}} = \\frac{\\partial L}{\\partial o^{(a)}_{k}} \\frac{\\partial o^{(a)}_{k}}{\\partial b^{(2)}_{k}} = \\frac{\\partial L}{\\partial o^{(a)}_{k}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{(2)}}_{m \\times d_h} = \\frac{\\partial L}{\\partial o^{a}}_{m \\times 1} \\cdot [h^{(s)}]^T_{1\\times d_h}\\\\\n",
    "\\frac{\\partial L}{\\partial b^{(2)}}_{m \\times 1} = \\frac{\\partial L}{\\partial o^{(a)}}_{m \\times 1}\n",
    "\\end{equation}\n",
    "\n",
    "```python\n",
    "grad_b2 = np.copy(grad_oa)\n",
    "grad_W2 = grad_oa * hs.T\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial h^{(s)}_{j}} = \\frac{\\partial L}{\\partial o^{(a)}_{k}} \\frac{\\partial o^{(a)}_{k}}{\\partial h^{(s)}_{j}} = \\frac{\\partial L}{\\partial o^{(a)}_{k}} W^{(2)}_{kj}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial h^{(s)}_{j}}_{d_h \\times 1} = [W^{(2)}]^T_{d_h \\times m} \\cdot \\frac{\\partial L}{\\partial o^{(a)}_{k}}_{m \\times 1}\n",
    "\\end{equation}\n",
    "\n",
    "~~~python\n",
    "grad_hs = W2.T * grad_oa\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial h^{(a)}_{j}} = \\frac{\\partial L}{\\partial h^{(s)}_{j}} \\frac{\\partial h^{(s)}_{j}}{\\partial h^{(a)}_{j}} = \\frac{\\partial L}{\\partial h^{(s)}_{j}} \\cdot 1_{x>0}(h^{(a)}_j)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial h^{(a)}_{j}}_{d_h \\times 1} = \\frac{\\partial L}{\\partial h^{(s)}_{j}}_{d_h \\times 1} \\cdot 1_{x>0}(h^{(a)}_j)_{d_h \\times 1}\n",
    "\\end{equation}\n",
    "\n",
    "~~~python\n",
    "rect = np.zeros(grad_hs.shape)\n",
    "rect[ha>0] = 1\n",
    "grad_ha = grad_hs * rect\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{(1)}_{ji}} = \\frac{\\partial L}{\\partial h^{(a)}_{j}} \\frac{\\partial h^{(a)}_{j}}{\\partial W^{(1)}_{ji}} = \\frac{\\partial L}{\\partial h^{(a)}_{j}} h_i^{(0)}\\\\\n",
    "\\frac{\\partial L}{\\partial b^{(1)}_{j}} = \\frac{\\partial L}{\\partial h^{(a)}_{j}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{(1)}}_{d_h \\times d} = \\frac{\\partial L}{\\partial h^{(a)}}_{d_h\\times 1} \\cdot [h^{(0)}]^T_{1\\times d}\\\\\n",
    "\\frac{\\partial L}{\\partial b^{(1)}}_{d_h\\times 1} = \\frac{\\partial L}{\\partial h^{(a)}}_{d_h\\times 1}\n",
    "\\end{equation}\n",
    "\n",
    "~~~python\n",
    "grad_W1 = grad_ha * h0.T\n",
    "grad_b1 = np.copy(grad_ha)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial x_{i}} = \\frac{\\partial L}{\\partial h^{(a)}_{j}} \\frac{\\partial h^{(a)}_{j}}{\\partial h^{(0)}_i}\\frac{\\partial h^{(0)}_{i}}{\\partial x_i} = \\frac{\\partial L}{\\partial h^{(a)}_{j}} \\cdot W^{(1)}_{ji} \\cdot 1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.\n",
    "\n",
    "L'ajout d'un terme de régularisation change seulement les gradients pour $W^{(1)}$ et $W^{(2)}$. Ils deviendront\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{(2)}}_{m \\times d_h} = \\frac{\\partial L}{\\partial o^{a}}_{m \\times 1} \\cdot [h^{(s)}]^T_{1\\times d_h} + \\lambda_{21}(-1_{x<0}(W^{(2)}) + 1_{x>0}(W^{(2)}))_{m\\times d_h} + 2\\lambda_{22}W^{(2)}_{m\\times d_h}\\\\\n",
    "\\frac{\\partial L}{\\partial W^{(1)}}_{d_h \\times d} = \\frac{\\partial L}{\\partial h^{a}}_{d_h \\times 1} \\cdot [h^{(0)}]^T_{1\\times d} + \\lambda_{11}(-1_{x<0}(W^{(1)}) + 1_{x>0}(W^{(1)}))_{d_h\\times d} + 2\\lambda_{12}W^{(1)}_{d_h\\times d}\n",
    "\\end{equation}\n",
    "\n",
    "~~~python\n",
    "rect = np.ones(W2.shape)\n",
    "rect[W2<0] = -1.\n",
    "grad_W2 = grad_oa * hs.T + lambda21 * rect + 2 * lambda22 * W2\n",
    "rect = np.ones(W1.shape)\n",
    "rect[W1<0] = -1.\n",
    "grad_W1 = grad_ha * h0.T + lambda11 * rect + 2 * lambda12 * W1\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Practical Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 1:\n",
    "        contributions = np.exp(x - np.max(x))\n",
    "        reponse = contributions / np.sum(contributions)\n",
    "    if x.ndim == 2:\n",
    "        contributions = np.exp(x - np.max(x, axis = 0))\n",
    "        reponse = contributions / np.sum(contributions, axis = 0)\n",
    "    return reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2]\n",
      " [0.2]\n",
      " [0.2]\n",
      " [0.2]\n",
      " [0.2]]\n"
     ]
    }
   ],
   "source": [
    "a = np.ones((5,1))\n",
    "print(softmax(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    reponse = np.zeros(x.shape)\n",
    "    reponse[x>0] = x[x>0]\n",
    "    return reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, input_dim = 2, hidden_dim = 9, output_dim = 2, lambda11 = 0.01, \n",
    "                lambda12 = 0.01, lambda21 = 0.01, lambda22 = 0.01):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lambda11 = lambda11\n",
    "        self.lambda12 = lambda12\n",
    "        self.lambda21 = lambda21\n",
    "        self.lambda22 = lambda22\n",
    "        \n",
    "    def initialize_parameters(self):\n",
    "        np.random.seed(3)\n",
    "        self.W1 = np.random.rand(self.hidden_dim,self.input_dim)*(2/np.sqrt(self.hidden_dim)) - (1./np.sqrt(self.hidden_dim))\n",
    "        self.W2 = np.random.rand(self.output_dim,self.hidden_dim)*(2/np.sqrt(self.output_dim)) - (1./np.sqrt(self.output_dim))\n",
    "        self.b1 = np.zeros((self.hidden_dim,1))\n",
    "        self.b2 = np.zeros((self.output_dim,1))\n",
    "        \n",
    "    def train(self,x,target):\n",
    "        self.h0 = x.reshape((self.input_dim,1))\n",
    "        self.target = target.reshape((self.output_dim,1))\n",
    "        self.fprop()\n",
    "        self.bprop()\n",
    "    \n",
    "    def fprop(self):\n",
    "        self.ha = self.b1 + np.matmul(self.W1, self.h0)\n",
    "        self.hs = relu(self.ha)\n",
    "        self.oa = self.b2 + np.matmul(self.W2, self.hs)\n",
    "        self.os = softmax(self.oa)\n",
    "        self.loss = -np.log(self.os[self.target == 1])\n",
    "        \n",
    "    def bprop(self):\n",
    "        self.grad_oa = np.copy(self.os)\n",
    "        self.grad_oa[self.target == 1] -= 1\n",
    "        rect = np.ones(self.W2.shape)\n",
    "        rect[self.W2<0] = -1.\n",
    "        self.grad_W2 = np.matmul(self.grad_oa,self.hs.T) + self.lambda21 * rect + 2 * self.lambda22 * self.W2\n",
    "        self.grad_b2 = np.copy(self.grad_oa)\n",
    "        self.grad_hs = np.matmul(self.W2.T,self.grad_oa)\n",
    "        rect = np.zeros(self.grad_hs.shape)\n",
    "        rect[self.ha>0] = 1\n",
    "        self.grad_ha = self.grad_hs * rect\n",
    "        rect = np.ones(self.W1.shape)\n",
    "        rect[self.W1<0] = -1.\n",
    "        self.grad_W1 = np.matmul(self.grad_ha, self.h0.T) + self.lambda11 * rect + 2 * self.lambda12 * self.W1\n",
    "        self.grad_b1 = np.copy(self.grad_ha)\n",
    "    \n",
    "    def finite_diff_oa(self, epsilon = 0.0001):\n",
    "        finite_grad_oa = np.zeros(self.grad_oa.shape)\n",
    "        loss_init = self.loss\n",
    "        for i in range(self.output_dim):\n",
    "            self.oa[i,0] += epsilon\n",
    "            self.fprop_from_oa()\n",
    "            loss_final = self.loss\n",
    "            self.oa[i,0] -= epsilon\n",
    "            finite_grad_oa[i,0] = (loss_final - loss_init) / epsilon\n",
    "        return finite_grad_oa / self.grad_oa\n",
    "            \n",
    "    def fprop_from_oa(self):\n",
    "        self.os = softmax(self.oa)\n",
    "        self.loss = -np.log(self.os[self.target == 1])\n",
    "        \n",
    "    def finite_diff_W2(self, epsilon = 0.00001):\n",
    "        finite_grad_W2 = np.zeros(self.grad_W2.shape)\n",
    "        loss_init = self.loss\n",
    "        for i in range(self.output_dim):\n",
    "            for j in range(self.hidden_dim):\n",
    "                self.W2[i,j] += epsilon\n",
    "                self.fprop_from_W2()\n",
    "                loss_final = self.loss\n",
    "                self.W2[i,j] -= epsilon\n",
    "                finite_grad_W2[i,j] = (loss_final - loss_init) / epsilon\n",
    "        return finite_grad_W2 / self.grad_W2\n",
    "            \n",
    "    def fprop_from_W2(self):\n",
    "        self.oa = self.b2 + np.matmul(self.W2, self.hs)\n",
    "        self.os = softmax(self.oa)\n",
    "        self.loss = -np.log(self.os[self.target == 1])\n",
    "        \n",
    "    def finite_diff_W1(self, epsilon = 0.00001):\n",
    "        finite_grad_W1 = np.zeros(self.grad_W1.shape)\n",
    "        loss_init = self.loss\n",
    "        for i in range(self.hidden_dim):\n",
    "            for j in range(self.input_dim):\n",
    "                self.W1[i,j] += epsilon\n",
    "                self.fprop_from_W1()\n",
    "                loss_final = self.loss\n",
    "                self.W1[i,j] -= epsilon\n",
    "                finite_grad_W1[i,j] = (loss_final - loss_init) / epsilon\n",
    "        return finite_grad_W1\n",
    "    \n",
    "    def fprop_from_W1(self):\n",
    "        self.fprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 4,-7,2,-3])\n",
    "target = np.array([1,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = NeuralNetwork(input_dim=5, hidden_dim=7, output_dim=4)\n",
    "N.initialize_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.74493366 -0.12646335  0.01115682 -0.0127687  -0.10876211 -0.0118696\n",
      "  -0.0106212 ]\n",
      " [ 0.18370878  0.0239114  -0.01084627  0.01721068  0.03717483 -0.01433024\n",
      "  -0.01444045]\n",
      " [ 0.20701384  0.02594618 -0.01597243  0.0102807   0.00970918 -0.01032829\n",
      "  -0.01275648]\n",
      " [ 0.32468626  0.07455298  0.01382186  0.01378361  0.02732084  0.0133627\n",
      "  -0.01320303]]\n",
      "[[ 0.97562697  1.10333582  0.         -0.          0.85749706 -0.\n",
      "  -0.        ]\n",
      " [ 1.07956062  1.59236052 -0.          0.          0.6845972  -0.\n",
      "  -0.        ]\n",
      " [ 1.05269358  1.6124918  -0.          0.          2.88022401 -0.\n",
      "  -0.        ]\n",
      " [ 0.95641079  0.79967413  0.          0.          1.45855154  0.\n",
      "  -0.        ]]\n"
     ]
    }
   ],
   "source": [
    "N.train(a,target)\n",
    "print(N.grad_W2)\n",
    "test = N.finite_diff_W2()\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
