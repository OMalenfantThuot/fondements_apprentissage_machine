{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devoir 3 pour IFT6390 - Fondements de l'apprentissage machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Par Olivier Malenfant-Thuot\n",
    "Matricule: 1012818"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Theoretical Part: Derivatives and relationships between basic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{2}(\\tanh(x/2)+1) = \\frac{1}{2}(\\frac{e^{x/2}-e^{-x/2}}{e^{x/2}+e^{-x/2}}+1) = \n",
    "\\frac{1}{2}(\\frac{e^{x/2}-e^{-x/2}}{e^{x/2}+e^{-x/2}}+\\frac{e^{x/2}+e^{-x/2}}{e^{x/2}+e^{-x/2}}) = \\frac{1}{2}(\\frac{2e^{x/2}}{e^{x/2}+e^{-x/2}}) = \\frac{2e^{x/2}(e^{x/2}+e^{-x/2})}{(e^{x/2}+e^{-x/2})^2} = \\frac{e^x+1}{e^x+2+e^{-x}} \\times \\frac{(e^x+1)^{-1}}{(e^x+1)^{-1}} = \\frac{1}{1+e^{-x}} = \\text{sigmoid}(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "\\begin{equation}\n",
    "\\ln(\\text{sigmoid}(x)) = \\ln(\\frac{1}{1+e^{-x}}) = \\ln(1) - \\ln(1+e^{-x}) = 0-\\ln(1+e^{-x})=-\\ln(1+e^{-x}) = -\\text{softplus}(-x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial x}\\text{sigmoid}(x) = \\frac{\\partial}{\\partial x}\\frac{1}{1+e^{-x}} = -(1+e^{-x})^{-2}(-e^{-x}) = \\frac{e^{-x}}{(1+e^{-x})^2} = \\frac{1}{1+e^{-x}}\\frac{e^{-x}}{1+e^{-x}} = \\frac{1}{1+e^{-x}}\\frac{1-1+e^{-x}}{1+e^{-x}} = \\frac{1}{1+e^{-x}}(\\frac{1+e^{-x}}{1+e^{-x}} - \\frac{1}{1+e^{-x}}) = \\frac{1}{1+e^{-x}}(1-\\frac{1}{1+e^{-x}}) =\\text{sigmoid}(x) (1 - \\text{sigmoid}(x))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial x}(\\tanh(x)) = \\frac{\\partial}{\\partial x} (\\frac{e^x - e^{-x}}{e^x + e^{-x}}) = \\frac{e^x + e^{-x}}{e^x + e^{-x}} - (e^x - e^{-x})(e^x + e^{-x})^{-2}(e^x - e^{-x}) = 1 - (\\frac{e^x - e^{-x}}{e^x + e^{-x}})^2 = 1 - \\tanh(x)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{sign}(x) = 1_{x>0}(x) - 1_{x<0}(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{abs'}(x) = \\text{sign(x)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{rect'}(x) = 1_{x>0}(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L_2}{\\partial x} = \\begin{bmatrix}\\frac{\\partial}{\\partial x_1}\\\\\n",
    "\\frac{\\partial}{\\partial x_2}\\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial x_d}\\end{bmatrix} L_2 = \\begin{bmatrix}2x_1\\\\\n",
    "2x_2\\\\\n",
    "\\vdots \\\\\n",
    "2x_d\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L_1}{\\partial x} = \\begin{bmatrix}\\frac{\\partial}{\\partial x_1}\\\\\n",
    "\\frac{\\partial}{\\partial x_2}\\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial x_d}\\end{bmatrix} L_1 = \\begin{bmatrix}-1_{x<0}(x) + 1_{x>0}(x)\\\\\n",
    "-1_{x<0}(x) + 1_{x>0}(x)\\\\\n",
    "\\vdots \\\\\n",
    "-1_{x<0}(x) + 1_{x>0}(x)\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Gradient computation for parameters optimization in a neural net for multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "Le vecteur $b^{(1)}$ est de dimensions $d_h$.\n",
    "\n",
    "Le vecteur de préactivation de la couche cachée est donné par\n",
    "\n",
    "\\begin{equation}\n",
    "h^{(a)}(x) = b^{(1)} + W^{(1)}h^{(0)}(x)\n",
    "\\end{equation}\n",
    "\n",
    "avec $h^{(0)}(x) = x$.\n",
    "\n",
    "Pour calculer un élément particulier de rang $j$, on utilise\n",
    "\n",
    "\\begin{equation}\n",
    "h^{(a)}_j(x) =  b^{(1)}_j + \\sum\\limits_{i = 1}^d W^{(1)}_{ji}x_i.\n",
    "\\end{equation}\n",
    "\n",
    "Les éléments du  vecteur de sortie pour la couche cachée, $h^{(s)}(x)$ sont donnés par\n",
    "\n",
    "\\begin{equation}\n",
    "h^{(s)}_j(x) = \\max(0,h^{(a)}_j(x)).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "Les dimensions de la matrice $W^{(2)}$ est $(m,d_h)$ et celles du vecteur $b^{(2)}$ sont $(m,1)$. La formule d'activation de la couche de sortie est donnée par\n",
    "\n",
    "\\begin{equation}\n",
    "o^{(a)}(x) = b^{(2)} + W^{(2)}h^{(s)}(x).\n",
    "\\end{equation}\n",
    "\n",
    "Chaque élément $o^{(a)}_k(x)$ de ce vecteur est donné par\n",
    "\n",
    "\\begin{equation}\n",
    "o^{(a)}_k(x) = b^{(2)}_k + \\sum\\limits_{j = 1}^{d_h} W^{(2)}_{kj}h^{(s)}_j(x).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "Les éléments du vecteur de sortie $o^{(s)}_k(x)$ sont donnés par\n",
    "\n",
    "\\begin{equation}\n",
    "o^{(s)}_k(x) = \\text{softmax}(o^{(a)}_k(x)) = \\ln(1 + \\exp(o^{(a)}_k(x))) = \\ln(1 + \\exp(b^{(2)}_k + \\sum\\limits_{j = 1}^{d_h} W^{(2)}_{kj}h^{(s)}_j(x))).\n",
    "\\end{equation}\n",
    "\n",
    "Les $o^{(s)}_k(x)$ sont tous positifs, car $(\\exp(x) > 0 \\text{ pour } x \\in \\rm I\\!R)$, donc $1 + \\exp(x) > 1$, donc $\\ln(1+\\exp(x)) > 0$, car $\\ln(x) > 0\\text{ pour } x > 1$.\n",
    "\n",
    "???Normalisation???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "La fonction de perte est donnée par\n",
    "\n",
    "\\begin{equation}\n",
    "L(x,y) = -\\ln(\\ln(1+\\exp(o^{(a)}(x)\\times \\text{onehot}_m(y))))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "\n",
    "Le risque empirique $\\hat{R}(x)$ du dataset $D_n$ est donné par\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{R}(x) = \\sum\\limits_{x \\in D_n} \\sum\\limits_{y =1}^m L(x,y).\n",
    "\\end{equation}\n",
    "\n",
    "L'ensemble $\\theta$ des paramètres du système est \n",
    "\n",
    "\\begin{equation}\n",
    "\\theta = \\{W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)}\\}.\n",
    "\\end{equation}\n",
    "\n",
    "$W^{(1)}$ est de dimension $(d_h,d)$, $b^{(1)}$ est de dimension $d_h$, $W^{(2)}$ est de dimension $(m,d_h)$ et $b^{(2)}$ est de dimension $m$. Le nombre de paramètres scalaires total est donc $n_\\theta = d_h(d+1) + m(d_h+1)$\n",
    "\n",
    "Les valeurs de paramètres optimisées seront obtenues par\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^* = \\text{argmin}_{\\theta}(-\\sum\\limits_{x \\in D_n} \\sum\\limits_{y=1}^m \\ln(\\ln(1+\\exp(o^{(a)}_y(x)\\times \\text{onehot}_m(y)))))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\n",
    "\n",
    "L'équation pour le batch gradient descent est\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla(\\theta) = \\frac{1}{n} \\sum\\limits_{x\\in D_n}[\\frac{\\partial}{\\partial \\theta}\\sum\\limits_{y=1}^m -\\ln(\\ln(1+\\exp(o^{(a)}_y(x)\\times \\text{onehot}_m(y)))) ],\\\\\n",
    "\\theta \\leftarrow \\theta - \\eta\\nabla(\\theta).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\n",
    "\n",
    "À voir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\n",
    "\n",
    "```python\n",
    "grad_oa = os\n",
    "grad_oa[np.argmax(os)] -= 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{(2)}_{kj}} = \\frac{\\partial L}{\\partial o^{(a)}_{k}} \\frac{\\partial o^{(a)}_{k}}{\\partial W^{(2)}_{kj}} = \\frac{\\partial L}{\\partial o^{(a)}_{k}} h^{(s)}_j\\\\\n",
    "\\frac{\\partial L}{\\partial b^{(2)}_{k}} = \\frac{\\partial L}{\\partial o^{(a)}_{k}} \\frac{\\partial o^{(a)}_{k}}{\\partial b^{(2)}_{k}} = \\frac{\\partial L}{\\partial o^{(a)}_{k}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{(2)}}_{m \\times d_h} = \\frac{\\partial L}{\\partial o^{a}}_{m \\times 1} \\cdot [h^{(s)}]^T_{1\\times d_h}\\\\\n",
    "\\frac{\\partial L}{\\partial b^{(2)}}_{m \\times 1} = \\frac{\\partial L}{\\partial o^{(a)}}_{m \\times 1}\n",
    "\\end{equation}\n",
    "\n",
    "```python\n",
    "grad_b2 = np.copy(grad_oa)\n",
    "grad_W2 = grad_oa * hs.T\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial h^{(s)}_{j}} = \\frac{\\partial L}{\\partial o^{(a)}_{k}} \\frac{\\partial o^{(a)}_{k}}{\\partial h^{(s)}_{j}} = \\frac{\\partial L}{\\partial o^{(a)}_{k}} W^{(2)}_{kj}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial h^{(s)}_{j}}_{d_h \\times 1} = [W^{(2)}]^T_{d_h \\times m} \\cdot \\frac{\\partial L}{\\partial o^{(a)}_{k}}_{m \\times 1}\n",
    "\\end{equation}\n",
    "\n",
    "~~~python\n",
    "grad_hs = W2.T * grad_oa\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial h^{(a)}_{j}} = \\frac{\\partial L}{\\partial h^{(s)}_{j}} \\frac{\\partial h^{(s)}_{j}}{\\partial h^{(a)}_{j}} = \\frac{\\partial L}{\\partial h^{(s)}_{j}} \\cdot 1_{x>0}(h^{(a)}_j)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial h^{(a)}_{j}}_{d_h \\times 1} = \\frac{\\partial L}{\\partial h^{(s)}_{j}}_{d_h \\times 1} \\cdot 1_{x>0}(h^{(a)}_j)_{d_h \\times 1}\n",
    "\\end{equation}\n",
    "\n",
    "~~~python\n",
    "rect = np.zeros(grad_hs.shape)\n",
    "rect[ha>0] = 1\n",
    "grad_ha = grad_hs * rect\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{(1)}_{ji}} = \\frac{\\partial L}{\\partial h^{(a)}_{j}} \\frac{\\partial h^{(a)}_{j}}{\\partial W^{(1)}_{ji}} = \\frac{\\partial L}{\\partial h^{(a)}_{j}} h_i^{(0)}\\\\\n",
    "\\frac{\\partial L}{\\partial b^{(1)}_{j}} = \\frac{\\partial L}{\\partial h^{(a)}_{j}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{(1)}}_{d_h \\times d} = \\frac{\\partial L}{\\partial h^{(a)}}_{d_h\\times 1} \\cdot [h^{(0)}]^T_{1\\times d}\\\\\n",
    "\\frac{\\partial L}{\\partial b^{(1)}}_{d_h\\times 1} = \\frac{\\partial L}{\\partial h^{(a)}}_{d_h\\times 1}\n",
    "\\end{equation}\n",
    "\n",
    "~~~python\n",
    "grad_W1 = grad_ha * h0.T\n",
    "grad_b1 = np.copy(grad_ha)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial x_{i}} = \\frac{\\partial L}{\\partial h^{(a)}_{j}} \\frac{\\partial h^{(a)}_{j}}{\\partial h^{(0)}_i}\\frac{\\partial h^{(0)}_{i}}{\\partial x_i} = \\frac{\\partial L}{\\partial h^{(a)}_{j}} \\cdot W^{(1)}_{ji} \\cdot 1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.\n",
    "\n",
    "L'ajout d'un terme de régularisation change seulement les gradients pour $W^{(1)}$ et $W^{(2)}$. Ils deviendront\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{(2)}}_{m \\times d_h} = \\frac{\\partial L}{\\partial o^{a}}_{m \\times 1} \\cdot [h^{(s)}]^T_{1\\times d_h} + \\lambda_{21}(-1_{x<0}(W^{(2)}) + 1_{x>0}(W^{(2)}))_{m\\times d_h} + 2\\lambda_{22}W^{(2)}_{m\\times d_h}\\\\\n",
    "\\frac{\\partial L}{\\partial W^{(1)}}_{d_h \\times d} = \\frac{\\partial L}{\\partial h^{a}}_{d_h \\times 1} \\cdot [h^{(0)}]^T_{1\\times d} + \\lambda_{11}(-1_{x<0}(W^{(1)}) + 1_{x>0}(W^{(1)}))_{d_h\\times d} + 2\\lambda_{12}W^{(1)}_{d_h\\times d}\n",
    "\\end{equation}\n",
    "\n",
    "~~~python\n",
    "rect = np.ones(W2.shape)\n",
    "rect[W2<0] = -1.\n",
    "grad_W2 = grad_oa * hs.T + lambda21 * rect + 2 * lambda22 * W2\n",
    "rect = np.ones(W1.shape)\n",
    "rect[W1<0] = -1.\n",
    "grad_W1 = grad_ha * h0.T + lambda11 * rect + 2 * lambda12 * W1\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 5\n",
    "dh = 7\n",
    "\n",
    "grad_oa= np.ones((m,1))\n",
    "hs = np.ones((1,dh))\n",
    "grad_W2 = grad_oa * hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(grad_W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4]\n",
      "[4 4]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([4,3])\n",
    "b = a * 1\n",
    "a[1] += 1\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ha = np.array([1, 2, 3, -2, -4, 6])\n",
    "ha.shape\n",
    "\n",
    "np.zeros(ha.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3., 0., 0., 6.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.maximum(ha,np.zeros(ha.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "grad = np.zeros(ha.shape)\n",
    "grad[ha>0] = 1\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.39724442 -0.07467383 -0.4837776   0.32199726 -0.31546464  0.24474138]\n",
      " [ 0.37338073 -0.05939823 -0.20622889  0.15768532 -0.02983222  0.48093379]\n",
      " [ 0.02392249 -0.26874833  0.17679754 -0.2450788  -0.16357716  0.07564577]\n",
      " [-0.39319279 -0.48694134  0.08791934  0.37566871 -0.04758637 -0.03888703]]\n"
     ]
    }
   ],
   "source": [
    "W2 = np.random.rand(4,6)-0.5\n",
    "print(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1. -1. -1.  1. -1.  1.]\n",
      " [ 1. -1. -1.  1. -1.  1.]\n",
      " [ 1. -1.  1. -1. -1.  1.]\n",
      " [-1. -1.  1.  1. -1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "rect = np.ones(W2.shape)\n",
    "rect[W2<0] = -1.\n",
    "print(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
