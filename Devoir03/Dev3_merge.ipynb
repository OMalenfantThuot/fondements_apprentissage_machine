{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devoir 3 pour IFT6390 - Fondements de l'apprentissage machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Par Olivier Malenfant-Thuot, Jimmy Leroux, Nicolas Laliberté\n",
    "Matricules: 1012818, 1024610, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Theoretical Part: Derivatives and relationships between basic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{2}(\\tanh(x/2)+1) = \\frac{1}{2}(\\frac{e^{x/2}-e^{-x/2}}{e^{x/2}+e^{-x/2}}+1) = \n",
    "\\frac{1}{2}(\\frac{e^{x/2}-e^{-x/2}}{e^{x/2}+e^{-x/2}}+\\frac{e^{x/2}+e^{-x/2}}{e^{x/2}+e^{-x/2}}) = \\frac{1}{2}(\\frac{2e^{x/2}}{e^{x/2}+e^{-x/2}}) = \\frac{2e^{x/2}(e^{x/2}+e^{-x/2})}{(e^{x/2}+e^{-x/2})^2} = \\frac{e^x+1}{e^x+2+e^{-x}} \\times \\frac{(e^x+1)^{-1}}{(e^x+1)^{-1}} = \\frac{1}{1+e^{-x}} = \\text{sigmoid}(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "\\begin{equation}\n",
    "\\ln(\\text{sigmoid}(x)) = \\ln(\\frac{1}{1+e^{-x}}) = \\ln(1) - \\ln(1+e^{-x}) = 0-\\ln(1+e^{-x})=-\\ln(1+e^{-x}) = -\\text{softplus}(-x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial x}\\text{sigmoid}(x) = \\frac{\\partial}{\\partial x}\\frac{1}{1+e^{-x}} = -(1+e^{-x})^{-2}(-e^{-x}) = \\frac{e^{-x}}{(1+e^{-x})^2} = \\frac{1}{1+e^{-x}}\\frac{e^{-x}}{1+e^{-x}} = \\frac{1}{1+e^{-x}}\\frac{1-1+e^{-x}}{1+e^{-x}} = \\frac{1}{1+e^{-x}}(\\frac{1+e^{-x}}{1+e^{-x}} - \\frac{1}{1+e^{-x}}) = \\frac{1}{1+e^{-x}}(1-\\frac{1}{1+e^{-x}}) =\\text{sigmoid}(x) (1 - \\text{sigmoid}(x))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial x}(\\tanh(x)) = \\frac{\\partial}{\\partial x} (\\frac{e^x - e^{-x}}{e^x + e^{-x}}) = \\frac{e^x + e^{-x}}{e^x + e^{-x}} - (e^x - e^{-x})(e^x + e^{-x})^{-2}(e^x - e^{-x}) = 1 - (\\frac{e^x - e^{-x}}{e^x + e^{-x}})^2 = 1 - \\tanh(x)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{sign}(x) = 1_{x>0}(x) - 1_{x<0}(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{abs'}(x) = \\text{sign(x)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{rect'}(x) = 1_{x>0}(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L_2}{\\partial x} = \\begin{bmatrix}\\frac{\\partial}{\\partial x_1}\\\\\n",
    "\\frac{\\partial}{\\partial x_2}\\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial x_d}\\end{bmatrix} L_2 = \\begin{bmatrix}2x_1\\\\\n",
    "2x_2\\\\\n",
    "\\vdots \\\\\n",
    "2x_d\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L_1}{\\partial x} = \\begin{bmatrix}\\frac{\\partial}{\\partial x_1}\\\\\n",
    "\\frac{\\partial}{\\partial x_2}\\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial x_d}\\end{bmatrix} L_1 = \\begin{bmatrix}-1_{x<0}(x) + 1_{x>0}(x)\\\\\n",
    "-1_{x<0}(x) + 1_{x>0}(x)\\\\\n",
    "\\vdots \\\\\n",
    "-1_{x<0}(x) + 1_{x>0}(x)\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Gradient computation for parameters optimization in a neural net for multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "Le vecteur $b^{(1)}$ est de dimensions $d_h$.\n",
    "\n",
    "Le vecteur de préactivation de la couche cachée est donné par\n",
    "\n",
    "\\begin{equation}\n",
    "h^{(a)}(x) = b^{(1)} + W^{(1)}h^{(0)}(x)\n",
    "\\end{equation}\n",
    "\n",
    "avec $h^{(0)}(x) = x$.\n",
    "\n",
    "Pour calculer un élément particulier de rang $j$, on utilise\n",
    "\n",
    "\\begin{equation}\n",
    "h^{(a)}_j(x) =  b^{(1)}_j + \\sum\\limits_{i = 1}^d W^{(1)}_{ji}x_i.\n",
    "\\end{equation}\n",
    "\n",
    "Les éléments du  vecteur de sortie pour la couche cachée, $h^{(s)}(x)$ sont donnés par\n",
    "\n",
    "\\begin{equation}\n",
    "h^{(s)}_j(x) = \\max(0,h^{(a)}_j(x)).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "Les dimensions de la matrice $W^{(2)}$ est $(m,d_h)$ et celles du vecteur $b^{(2)}$ sont $(m,1)$. La formule d'activation de la couche de sortie est donnée par\n",
    "\n",
    "\\begin{equation}\n",
    "o^{(a)}(x) = b^{(2)} + W^{(2)}h^{(s)}(x).\n",
    "\\end{equation}\n",
    "\n",
    "Chaque élément $o^{(a)}_k(x)$ de ce vecteur est donné par\n",
    "\n",
    "\\begin{equation}\n",
    "o^{(a)}_k(x) = b^{(2)}_k + \\sum\\limits_{j = 1}^{d_h} W^{(2)}_{kj}h^{(s)}_j(x).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "Les éléments du vecteur de sortie $o^{(s)}_k(x)$ sont donnés par\n",
    "\n",
    "\\begin{equation}\n",
    "o^{(s)}_k(x) = \\text{softmax}(o^{(a)}_k(x)) = \\frac{e^{o^a_k}}{\\sum_{k'} e^{o^{a}_{k'}}}\n",
    "\\end{equation}\n",
    "\n",
    "Les $o^{(s)}_k(x)$ sont tous positifs, car $(\\exp(x) > 0 \\text{ pour } x \\in \\rm I\\!R)$. La somme des termes de $o^{(s)}$ est donc $\\frac{\\sum_{k} e^{o^{a}_{k}}}{\\sum_{k'} e^{o^{a}_{k'}}} = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "La fonction de perte est donnée par\n",
    "\n",
    "\\begin{equation}\n",
    "L(x,y) = -\\ln(o^s_y) = -o^a_y + \\ln(\\sum_k e^{o^{a}_k}),\\\\\n",
    "L(x,y) = -onehot_m(y) \\cdot o^a + \\ln(\\sum_k e^{o^{a}_k}).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\n",
    "\n",
    "Le risque empirique $\\hat{R}(x)$ du dataset $D_n$ est donné par\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{R}(\\theta, D_n) = \\frac{1}{n}\\sum\\limits_{x \\in D_n} L(x,y).\n",
    "\\end{equation}\n",
    "\n",
    "L'ensemble $\\theta$ des paramètres du système est \n",
    "\n",
    "\\begin{equation}\n",
    "\\theta = \\{W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)}\\}.\n",
    "\\end{equation}\n",
    "\n",
    "$W^{(1)}$ est de dimension $(d_h,d)$, $b^{(1)}$ est de dimension $d_h$, $W^{(2)}$ est de dimension $(m,d_h)$ et $b^{(2)}$ est de dimension $m$. Le nombre de paramètres scalaires total est donc $n_\\theta = d_h(d+1) + m(d_h+1)$\n",
    "\n",
    "Les valeurs de paramètres optimisées seront obtenues par\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^* = \\text{argmin}_{\\theta}(\\hat{R}(\\theta, D_n)).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\n",
    "\n",
    "L'équation pour le batch gradient descent est\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla(\\theta) = \\frac{\\partial}{\\partial \\theta} \\hat{R}(\\theta, D_n),\\\\\n",
    "\\theta \\leftarrow \\theta - \\eta\\nabla(\\theta).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial o^a} = \\frac{\\partial}{\\partial o^a} (-onehot_m(y) \\cdot o^a + \\ln(\\sum_k e^{o^{a}_k})) = -onehot_m(y) + \\frac{\\partial}{\\partial o^a}(\\ln\\sum_k e^{o^a_k}) = -onehot_m(y) + \\frac{1}{\\sum_k e^{o^a_k}}\\frac{\\partial}{\\partial o^a}(\\sum_k e^{o^a_k}) = -onehot_m(y) + \\frac{1}{\\sum_k e^{o^a_k}}\\begin{bmatrix} e^{o^a_1}\\\\e^{o^a_2}\\\\\\vdots\\\\e^{o^a_m}\\end{bmatrix} = -onehot_m(y) + o^s\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\n",
    "\n",
    "```python\n",
    "grad_oa = np.copy(os)\n",
    "grad_oa[target == 1] -= 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{(2)}_{kj}} = \\frac{\\partial L}{\\partial o^{(a)}_{k}} \\frac{\\partial o^{(a)}_{k}}{\\partial W^{(2)}_{kj}} = \\frac{\\partial L}{\\partial o^{(a)}_{k}} h^{(s)}_j\\\\\n",
    "\\frac{\\partial L}{\\partial b^{(2)}_{k}} = \\frac{\\partial L}{\\partial o^{(a)}_{k}} \\frac{\\partial o^{(a)}_{k}}{\\partial b^{(2)}_{k}} = \\frac{\\partial L}{\\partial o^{(a)}_{k}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{(2)}}_{m \\times d_h} = \\frac{\\partial L}{\\partial o^{a}}_{m \\times 1} \\cdot [h^{(s)}]^T_{1\\times d_h}\\\\\n",
    "\\frac{\\partial L}{\\partial b^{(2)}}_{m \\times 1} = \\frac{\\partial L}{\\partial o^{(a)}}_{m \\times 1}\n",
    "\\end{equation}\n",
    "\n",
    "```python\n",
    "grad_b2 = np.copy(grad_oa)\n",
    "grad_W2 = grad_oa * hs.T\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial h^{(s)}_{j}} = \\frac{\\partial L}{\\partial o^{(a)}_{k}} \\frac{\\partial o^{(a)}_{k}}{\\partial h^{(s)}_{j}} = \\frac{\\partial L}{\\partial o^{(a)}_{k}} W^{(2)}_{kj}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial h^{(s)}_{j}}_{d_h \\times 1} = [W^{(2)}]^T_{d_h \\times m} \\cdot \\frac{\\partial L}{\\partial o^{(a)}_{k}}_{m \\times 1}\n",
    "\\end{equation}\n",
    "\n",
    "~~~python\n",
    "grad_hs = W2.T * grad_oa\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial h^{(a)}_{j}} = \\frac{\\partial L}{\\partial h^{(s)}_{j}} \\frac{\\partial h^{(s)}_{j}}{\\partial h^{(a)}_{j}} = \\frac{\\partial L}{\\partial h^{(s)}_{j}} \\cdot 1_{x>0}(h^{(a)}_j)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial h^{(a)}_{j}}_{d_h \\times 1} = \\frac{\\partial L}{\\partial h^{(s)}_{j}}_{d_h \\times 1} \\cdot 1_{x>0}(h^{(a)}_j)_{d_h \\times 1}\n",
    "\\end{equation}\n",
    "\n",
    "~~~python\n",
    "rect = np.zeros(grad_hs.shape)\n",
    "rect[ha>0] = 1\n",
    "grad_ha = grad_hs * rect\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{(1)}_{ji}} = \\frac{\\partial L}{\\partial h^{(a)}_{j}} \\frac{\\partial h^{(a)}_{j}}{\\partial W^{(1)}_{ji}} = \\frac{\\partial L}{\\partial h^{(a)}_{j}} h_i^{(0)}\\\\\n",
    "\\frac{\\partial L}{\\partial b^{(1)}_{j}} = \\frac{\\partial L}{\\partial h^{(a)}_{j}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{(1)}}_{d_h \\times d} = \\frac{\\partial L}{\\partial h^{(a)}}_{d_h\\times 1} \\cdot [h^{(0)}]^T_{1\\times d}\\\\\n",
    "\\frac{\\partial L}{\\partial b^{(1)}}_{d_h\\times 1} = \\frac{\\partial L}{\\partial h^{(a)}}_{d_h\\times 1}\n",
    "\\end{equation}\n",
    "\n",
    "~~~python\n",
    "grad_W1 = grad_ha * h0.T\n",
    "grad_b1 = np.copy(grad_ha)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial x_{i}} = \\frac{\\partial L}{\\partial h^{(a)}_{j}} \\frac{\\partial h^{(a)}_{j}}{\\partial h^{(0)}_i}\\frac{\\partial h^{(0)}_{i}}{\\partial x_i} = \\frac{\\partial L}{\\partial h^{(a)}_{j}} \\cdot W^{(1)}_{ji} \\cdot 1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.\n",
    "\n",
    "L'ajout d'un terme de régularisation change seulement les gradients pour $W^{(1)}$ et $W^{(2)}$. Ils deviendront\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{(2)}}_{m \\times d_h} = \\frac{\\partial L}{\\partial o^{a}}_{m \\times 1} \\cdot [h^{(s)}]^T_{1\\times d_h} + \\lambda_{21}(-1_{x<0}(W^{(2)}) + 1_{x>0}(W^{(2)}))_{m\\times d_h} + 2\\lambda_{22}W^{(2)}_{m\\times d_h}\\\\\n",
    "\\frac{\\partial L}{\\partial W^{(1)}}_{d_h \\times d} = \\frac{\\partial L}{\\partial h^{a}}_{d_h \\times 1} \\cdot [h^{(0)}]^T_{1\\times d} + \\lambda_{11}(-1_{x<0}(W^{(1)}) + 1_{x>0}(W^{(1)}))_{d_h\\times d} + 2\\lambda_{12}W^{(1)}_{d_h\\times d}\n",
    "\\end{equation}\n",
    "\n",
    "~~~python\n",
    "grad_W2 = grad_oa * hs.T + lambda21 * np.sign(W2) + 2 * lambda22 * W2\n",
    "grad_W1 = grad_ha * h0.T + lambda11 * np.sign(W1) + 2 * lambda12 * W1\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Practical Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import utils.mnist_reader as mnist_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(self, X):\n",
    "    max_ = X.max(axis=0)\n",
    "    O_s = np.exp(X - max_) / np.sum(np.exp(X - max_), axis=0)\n",
    "    return O_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Implement the neural network with one hidden layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers=[], lams=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001],\n",
    "            minibatch_size=16):\n",
    "        \"\"\"\n",
    "        Initialize the class attributes\n",
    "        \"\"\"\n",
    "\n",
    "        self.layers = layers # Number of nodes per layers\n",
    "        self.parameters = {} # Dictionnary of all the model parameters\n",
    "        self.lams = lams # Regularization hyperparameters lams=[lam11,lam12,lam21,lam22]\n",
    "        self.K = minibatch_size # Minibatch size\n",
    "        self.initialize_parameters() # Parameters initialization\n",
    "        \n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialization of the model's parameters\n",
    "        \"\"\"\n",
    "        \n",
    "        num_layer = len(self.layers)\n",
    "        for i in range(1, num_layer):\n",
    "            n_c = 1./np.sqrt(self.layers[i])\n",
    "            self.parameters[\"W\" + str(i)] = np.ones((self.layers[i],\n",
    "                self.layers[i-1])) * np.random.uniform(-n_c, n_c,\n",
    "                    (self.layers[i],self.layers[i-1]))\n",
    "            self.parameters[\"b\" + str(i)] = np.zeros((self.layers[i],1))\n",
    "        \n",
    "    def train(self,x,target):\n",
    "        self.h0 = x.reshape((self.input_dim,1))\n",
    "        self.target = target.reshape((self.output_dim,1))\n",
    "        self.fprop()\n",
    "        self.bprop()\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation method. It propagated X through the network.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: Input matrix we wish to propagate. Shape: (dim, num_exemple)\n",
    "\n",
    "        Returns:\n",
    "        values: Dictionary of the intermediate values at each step of the propagation.\n",
    "        \"\"\"\n",
    "\n",
    "        h_a = np.dot(self.parameters[\"W1\"], X) + self.parameters[\"b1\"]\n",
    "        h_s = relu(h_a)\n",
    "        O_a = np.dot(self.parameters[\"W2\"], h_s) + self.parameters[\"b2\"]\n",
    "        O_s = softmax(O_a)\n",
    "        values = {\"h_a\":h_a, \"h_s\":h_s, \"O_a\":O_a, \"O_s\":O_s, \"X\":X}\n",
    "        return values\n",
    "        \n",
    "    def bprop(self, values, Y):\n",
    "        \"\"\"\n",
    "        Method performing the backpropagation for the model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        values: Stored intermediate values of the forward propagation pass.\n",
    "        Y: Target values (of the training set). Shape: (num_class, num_exemple)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        grads: Dictionary containing the gradients of the parameters.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        grads = {}\n",
    "        dO_a = values[\"O_s\"] - Y\n",
    "        dW2 = np.mean(dO_a[:,None,:] * values[\"h_s\"][None,:,:], axis=2) +\\\n",
    "            2 * self.lams[3] * self.parameters[\"W2\"] +\\\n",
    "            self.lams[2] * np.sign(self.parameters[\"W2\"])\n",
    "        db2 = np.mean(dO_a, axis=1, keepdims=True)\n",
    "        dh_s = np.sum(dO_a[:,None,:] * self.parameters[\"W2\"][:,:,None], axis=0)\n",
    "        dh_a = dh_s * 1. * (values[\"h_s\"]>0)\n",
    "        dW1 = np.mean(dh_a[:,None,:] * values[\"X\"][None,:,], axis=2) +\\\n",
    "            2 * self.lams[1] * self.parameters[\"W1\"] +\\\n",
    "            self.lams[0] * np.sign(self.parameters[\"W1\"])\n",
    "        db1 = np.mean(dh_a, axis=1, keepdims=True)\n",
    "        dx = np.sum(dh_a[:,None,:]*self.parameters[\"W1\"][:,:,None], axis=0)\n",
    "        grads = {\"dW2\":dW2, \"dW1\":dW1, \"db2\":db2, \"db1\":db1, \"dx\":dx}\n",
    "        return grads\n",
    "\n",
    "    def loss(self, X, Y, values): \n",
    "        \"\"\"\n",
    "        Calculate the loss value.\n",
    "        \"\"\"\n",
    "\n",
    "        loss = np.sum(-np.log(values[\"O_s\"])*Y)\n",
    "        loss += (self.lams[0] * np.sum(np.abs(self.parameters[\"W1\"])) +\\\n",
    "            self.lams[2] * np.sum(np.abs(self.parameters[\"W2\"])) +\\\n",
    "            self.lams[1] * np.sum(self.parameters[\"W1\"]**2) +\\\n",
    "            self.lams[3] * np.sum(self.parameters[\"W2\"]**2)) * X.shape[1]\n",
    "        return loss\n",
    "    \n",
    "    def accuracy(self, X, Y):\n",
    "        \"\"\"\n",
    "        Method evaluating the accuracy of the model.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: Input we wish to evaluate the performance on. Shape: (dim, num_exemple)\n",
    "        Y: The respective target for each of the exemples.\n",
    "            Shape: (num_class, num_exemple)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        acc: The accuracy of the model to predict the input X\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        pred = self.prediction(X)\n",
    "        num_correct = float(np.sum(pred==np.argmax(Y, axis=0)))\n",
    "        acc = num_correct/X.shape[1]\n",
    "        return acc\n",
    "    \n",
    "    def grad_check(self, X, Y, totest):\n",
    "        values = self.fprop(X)\n",
    "        grad = self.bprop(values, Y)\t\t\n",
    "        epsilon = 0.000001\n",
    "        dtest = np.zeros(grad[\"d\"+totest].shape)\n",
    "\n",
    "        for i in range(dtest.shape[0]):\n",
    "            for j in range(dtest.shape[1]):\n",
    "                self.parameters[totest][i,j] = self.parameters[totest][i,j] + epsilon\n",
    "                values = self.fprop(X)\n",
    "                loss1 = self.loss(X, Y, values)\n",
    "                self.parameters[totest][i,j] = self.parameters[totest][i,j] - 2 * epsilon\n",
    "                values = self.fprop(X)\n",
    "                loss2 = self.loss(X, Y, values)\n",
    "                dtest[i,j] = (loss1-loss2)/(2*epsilon)/X.shape[1]\n",
    "                self.parameters[totest][i,j] = self.parameters[totest][i,j] + epsilon\n",
    "        return dtest/grad[\"d\"+totest]\n",
    "    \n",
    "    def to_minibatch(self, X, Y, seed):\n",
    "        np.random.seed(seed)\n",
    "        inds = np.arange(X.shape[1])\n",
    "        np.random.shuffle(inds)\t\t\n",
    "        random_x = X[:,inds] \n",
    "        random_y = Y[:,inds]\n",
    "        complete_mini = X.shape[1] // self.K\n",
    "\n",
    "        minibatch = []\n",
    "        for i in range(complete_mini):\n",
    "            mini_x = random_x[:,i * self.K:(i + 1) * self.K]\n",
    "            mini_y = random_y[:,i * self.K:(i + 1) * self.K]\n",
    "            minibatch.append((mini_x, mini_y))\n",
    "\n",
    "        if X.shape[1]%self.K!=0:\n",
    "            mini_x = random_x[:,complete_mini * self.K:]\n",
    "            mini_y = random_y[:,complete_mini * self.K:]\n",
    "            minibatch.append((mini_x, mini_y))\n",
    "        return minibatch\n",
    "    \n",
    "    def train(self, dataset, num_epoch, lr=0.01):\n",
    "        acc_train = []\n",
    "        acc_valid = []\n",
    "        acc_test = []\n",
    "        loss_train = []\n",
    "        loss_valid = []\n",
    "        loss_test = []\n",
    "        for epoch in range(num_epoch):\n",
    "            minibatch = self.to_minibatch(dataset.train_x, dataset.train_y, epoch)\n",
    "            loss = 0\n",
    "            for mini in minibatch:\n",
    "                mini_x = mini[0]\n",
    "                mini_y = mini[1]\n",
    "                values = self.fprop(mini_x)\n",
    "                grad = self.bprop(values, mini_y)\n",
    "                self.update_param(grad,(lr / (1 + 4 * epoch / num_epoch)))\n",
    "                loss += self.loss(mini_x, mini_y, values)\n",
    "            acc_train.append(1-self.accuracy(dataset.train_x, dataset.train_y))\n",
    "            acc_test.append(1-self.accuracy(dataset.test_x, dataset.test_y))\n",
    "            print(loss)\n",
    "        return acc_train, acc_test\n",
    "\n",
    "    def update_param(self, grad, lambda_):\n",
    "        self.parameters[\"W1\"] = self.parameters[\"W1\"] - lambda_ * grad[\"dW1\"]\n",
    "        self.parameters[\"W2\"] = self.parameters[\"W2\"] - lambda_ * grad[\"dW2\"]\n",
    "        self.parameters[\"b1\"] = self.parameters[\"b1\"] - lambda_ * grad[\"db1\"]\n",
    "        self.parameters[\"b2\"] = self.parameters[\"b2\"] - lambda_ * grad[\"db2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self,X,Y,numclass):\n",
    "        self.X = X.T\n",
    "        self.Y = Y\n",
    "        self.train_x = 0\n",
    "        self.train_y = 0\n",
    "        self.valid_x = 0\n",
    "        self.valid_y = 0\n",
    "        self.test_x = 0\n",
    "        self.test_y = 0\t\n",
    "        self.numclass = numclass\n",
    "        self.toonehot()\n",
    "        self.split_and_randomize()\n",
    "\n",
    "    def toonehot(self):\t\n",
    "        onehot = np.zeros((self.numclass,len(self.Y)))\n",
    "        for j in range(len(self.Y)):\n",
    "            onehot[int(self.Y[j]),j] = 1.\n",
    "        self.Y = onehot\n",
    "\n",
    "    def split_and_randomize(self):\n",
    "        n_train = int(0.70 * self.X.shape[1])\n",
    "        n_valid = int(0.15 * self.X.shape[1])\n",
    "        inds = np.arange(self.X.shape[1])\n",
    "        np.random.shuffle(inds)\n",
    "        train_inds = inds[:n_train]\n",
    "        valid_inds = inds[n_train:n_train+n_valid]\n",
    "        test_inds = inds[n_train+n_valid:]\t\t\n",
    "        self.train_x = self.X[:,train_inds]\n",
    "        self.train_y = self.Y[:,train_inds]\n",
    "        mean_train = self.train_x.mean(axis=1, keepdims=True)\n",
    "        std_train = self.train_x.std(axis=1, keepdims=True)\n",
    "        self.train_x = (self.train_x - mean_train) / std_train\n",
    "        self.valid_x = (self.X[:,valid_inds] - mean_train) / std_train\n",
    "        self.valid_y = self.Y[:,valid_inds]\n",
    "        self.test_x = (self.X[:,test_inds] - mean_train) / std_train\n",
    "        self.test_y = self.Y[:,test_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import cercles data\n",
    "data = np.loadtxt(open('cercles.txt','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NeuralNetwork(layers=[2,2],minibatch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choix d'un point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.95280609 -0.30357956  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(data[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[20,0:2]\n",
    "Y = np.array([[1],[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'W2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-3a074036a0e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgrads_bprop\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-aa92c8d0519d>\u001b[0m in \u001b[0;36mfprop\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mh_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mh_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mO_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_s\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mO_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mO_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"h_a\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mh_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"h_s\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mh_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"O_a\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mO_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"O_s\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mO_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"X\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'W2'"
     ]
    }
   ],
   "source": [
    "values = network.fprop(X=X)\n",
    "grads_bprop= network.bprop(Y=Y, values=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
