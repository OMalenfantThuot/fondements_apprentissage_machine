{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Classifier. Maximum Likelihood for a multivariate Gaussian density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin, if necessary, by recalling the course notes on the [Bayes classifier](https://studium.umontreal.ca/pluginfile.php/4027948/mod_resource/content/4/7_bayes_classifier-en.pdf) and the principle of [maximum likelihood](https://studium.umontreal.ca/pluginfile.php/4003963/mod_resource/content/4/5_gaussian_distribution_en.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High level description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we are going to build a **multi-class Bayes classifier**. This means that instead of modeling $ p (\\mbox{class} | \\mbox{example}) $ (or $ p (y | x) $), we will instead use the Bayes equation\n",
    "\n",
    "$$ p (\\mbox{class} | \\mbox{example}) = \\frac{p (\\mbox{example} | \\mbox{class}) p (\\mbox {class})} {\\sum_{c'= 1}^{m} p_\\mbox{c'}(x) P_\\mbox{c'}} $$\n",
    "\n",
    "and model the different pieces. In fact, we just need to model the numerator since the denominator is a normalization constant. In addition, $ P_\\mbox{c '} = n_c / n $\n",
    "\n",
    "The term $ p (\\mbox{class}) $ represents the prior probability of a class, that is, our a priori belief - before we have seen a particular example - about the probability that an unknown example belongs to this class). We will represent this belief a priori for a class by the frequency of the latter in the training data: $ \\frac{n_c}{n} $ where $ n_c $ = number of examples of the class $ c $, and $ n $ = number of training examples.\n",
    "\n",
    "We will use **multivariate Gaussian densities** to model the different $ p (\\mbox{example} | \\mbox{class}) $. This means that for each class, we will assume that the \"true\" distribution $ p (\\mbox{example} | \\mbox{class}) $ has the form of a multivariate Gaussian for which we will try to learn the parameters $ \\mu $ and $ \\Sigma $. In practice, we will limit ourselves today to a particular case of this distribution: the one where we assume that the covariance matrix $ \\Sigma $ of each Gaussian is diagonal and that each element of this diagonal is the same, i.e. sigma_sq (<=> \"sigma square\" <=> $ \\sigma^2 $ <=> the variance). Thus we have a single parameter to control the expression of the covariance. It's easier (for us and for the computer) to calculate, but it also means that our model is less powerful.\n",
    "\n",
    "So we have a very simple parametric model. The parameters are the average $ \\mu $ (a vector of the same dimension as the dimension of the system input) and the variance $ \\sigma^2 $ (a single scalar in our simple model, which will multiply the identity matrix). Learning in this model will be done today by applying the **maximum likelihood principle**. For each class, we will find the values of the parameters that maximize the log-likelihood of the training data from this class:\n",
    "\n",
    "$$ \\log \\prod_i^n p(X = x_i) $$\n",
    "\n",
    "For an insotropic Gaussian of dimension $d$, the maximum likelihood estimators of $\\mu$ and $\\sigma^2$ are given by: \n",
    "\n",
    "$$\\mu_{ML} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$$\n",
    "\n",
    "$$\\sigma_{ML}^2 = \\frac{1}{nd} \\sum_{i=1}^{n} (x_i-\\mu_{MV})^T(x_i-\\mu_{MV})$$\n",
    "\n",
    "Having found the parameters that maximize the likelihood for each class, we can calculate each $ p (\\mbox{example} | \\mbox{class}) $. It is now sufficient to apply the Bayes rule in order to classify a new example. More precisely, we want to choose, for an example, the class that maximizes $ p(\\mbox{example} | \\mbox{class}) p(\\mbox{class}) $ or, equivalently, $ \\log (p (\\mbox{example } | \\mbox{class} ) p(\\mbox{class})) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the file `utilities.py` into the folder where your notebook files are located. It contains the useful functions you saw in the last class. You can thus use them without cluttering your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `gauss_ml` class:\n",
    " \n",
    "  - calculate sigma_sq ($ \\sigma^2 $), the variance in `gauss_ml.train`\n",
    "  - calculate the value of the Gaussian density function in `gauss_ml.compute_predictions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pylab inline\n",
    "import numpy as np\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gauss_ml:\n",
    "    def __init__(self,n_dims,cov_type=\"isotropic\"):\n",
    "        self.n_dims = n_dims\n",
    "        self.mu = np.zeros((1,n_dims))\n",
    "        self.cov_type = cov_type\n",
    "        \n",
    "        if cov_type==\"isotropic\":\n",
    "            self.sigma_sq = 1.0\n",
    "        if cov_type==\"full\":\n",
    "            pass\n",
    "\n",
    "\t# For a training set, the function should compute the ML estimator of the mean and the covariance matrix\n",
    "    def train(self, train_data):\n",
    "        self.mu = np.mean(train_data,axis=0)\n",
    "        if self.cov_type == \"isotropic\":\n",
    "            self.sigma_sq = ((1./ train_data.shape[0] * self.n_dims) * \n",
    "                             np.sum(np.matmul(np.transpose(train_data - self.mu), train_data - self.mu)))\n",
    "        if self.cov_type == \"full\":\n",
    "            pass\n",
    "\n",
    "\t# Returns a vector of size nb. of test ex. containing the log\n",
    "\t# probabilities of each test example under the model.\t\n",
    "    def compute_predictions(self, test_data):\n",
    "        \n",
    "        log_prob = -np.ones((test_data.shape[0],1))\n",
    "        print(self.mu)\n",
    "        if self.cov_type == \"isotropic\":\n",
    "            # the following line calculates log(normalization constant)\n",
    "            c = -self.n_dims * np.log(2*np.pi)/2 - self.n_dims*np.log(np.sqrt(self.sigma_sq))\n",
    "            for i, test_point in enumerate(test_data):\n",
    "                log_prob[i] = c - 1./2 * np.sum((test_point - self.mu)**2) / self.sigma_sq        \n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For class `classif_bayes`:\n",
    "\n",
    "  - complete `classif_bayes.compute_predictions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classif_bayes:\n",
    "\n",
    "    def __init__(self,models_ml, priors):\n",
    "        self.models_ml = models_ml\n",
    "        self.priors = priors\n",
    "        if len(self.models_ml) != len(self.priors):\n",
    "            print('The number of ML models must be equal to the number of priors!')\n",
    "        self.n_classes = len(self.models_ml)\n",
    "\t\t\t\n",
    "    # Returns a matrix of size nb. of test ex. times number of classes containing the log\n",
    "    # probabilities of each test example under each model, trained by ML.\t\n",
    "    def compute_predictions(self, test_data):\n",
    "\n",
    "        log_pred = np.empty((test_data.shape[0],self.n_classes))\n",
    "        for i in range(self.n_classes):\n",
    "            # Here we will have to use modeles_mv [i] and priors to fill in\n",
    "            # each column of log_pred (it's more efficient to do a entire column at a time)\n",
    "            log_pred[:,i] = np.transpose(model_ml[i].compute_predictions(test_data) * self.priors[i])\n",
    "        return log_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code by calculating the maximum per class and displaying the decision area graph using the functions in `utilities.py`` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[5.006 3.428 1.462 0.246]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,) (4,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-e3e6b930425a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#max_prob = np.argmax(log_prob, axis = 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels_ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miris_train1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miris_train1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workdir/fondements_apprentissage_machine/WEEK04/utilities.py\u001b[0m in \u001b[0;36mgridplot\u001b[0;34m(classifier, train, test, n_points)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mthegrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgrid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mygrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mthe_accounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthegrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mclassesPred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_accounts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-2aa53031aa58>\u001b[0m in \u001b[0;36mcompute_predictions\u001b[0;34m(self, test_data)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# Here we will have to use modeles_mv [i] and priors to fill in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# each column of log_pred (it's more efficient to do a entire column at a time)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mlog_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpriors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlog_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-bbdaa98ac859>\u001b[0m in \u001b[0;36mcompute_predictions\u001b[0;34m(self, test_data)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dims\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dims\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_sq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_point\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mlog_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_point\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_sq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,) (4,) "
     ]
    }
   ],
   "source": [
    "# Here we provide an example where we do not divide the data into train and test set.\n",
    "# it's up to you to do it like in demo 3\n",
    "iris=np.loadtxt('iris.txt')\n",
    "iris_train1=iris[0:50,:-1]\n",
    "iris_train2=iris[50:100,:-1]\n",
    "iris_train3=iris[100:150,:-1]\n",
    "\n",
    "# We create a model per class (using maximum likelihood)\n",
    "model_class1=gauss_ml(2)\n",
    "model_class2=gauss_ml(2)\n",
    "model_class3=gauss_ml(2)\n",
    "model_class1.train(iris_train1)\n",
    "model_class2.train(iris_train2)\n",
    "model_class3.train(iris_train3)\n",
    "\n",
    "# We create a list of all our models\n",
    "# We do the same thing for the priors\n",
    "# Here the priors are calculated exactly because we know the number of representatives per class. \n",
    "# Once you have created a train / test set, they need to be calculated exactly\n",
    "model_ml=[model_class1,model_class2,model_class3]\n",
    "priors=[0.3333,0.3333,0.3333]\n",
    "\n",
    "# We create our classifier with our list of Gaussian models and our priors\n",
    "classifier=classif_bayes(model_ml,priors)\n",
    "\n",
    "# we can now calculate the log-probabilities according to our models\n",
    "#log_prob=classifier.compute_predictions(iris_train3)\n",
    "\n",
    "# it now remains to calculate the maximum per class for the classification\n",
    "#max_prob = np.argmax(log_prob, axis = 1)\n",
    "print(classifier.models_ml[0].n_dims)\n",
    "utilities.gridplot(classifier, iris_train1[:,0:2], iris_train1[:,0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once you're done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change your code so that `gauss_ml` calculates a diagonal covariance matrix (where we estimate the variance for each component / trait of the input) or even a full covariance matrix.\n",
    "- The `numpy.cov` and` numpy.var` commands will probably be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
